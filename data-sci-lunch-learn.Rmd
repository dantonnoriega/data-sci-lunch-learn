---
title: "Introduction to Stan with R: rstanarm, brms, and rethinking"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

# Overview

- [Stan](https://mc-stan.org) 
- [Stan Interfaces](https://mc-stan.org/users/interfaces/)
  - Exists for R, pythong, julia, etc.
- Stan with R
  - [rstanarm](http://mc-stan.org/rstanarm/articles/)
  - [brms](https://github.com/paul-buerkner/brms)
  - [rethinking](https://github.com/rmcelreath/rethinking/tree/Experimental)

## Case Study: Multi-level Tadpoles

- Idea blatently stolen from Chapter 12 of (my favorite book) [Stastical Rethinking](https://xcelab.net/rm/statistical-rethinking/) by Richard McElreath
  - See the [online lecture](https://www.youtube.com/watch?v=AALYPv5xSos&t=3140s), which is part of his [Winter 2019 class](https://www.youtube.com/playlist?list=PLDcUM9US4XdNM4Edgs7weiyIguLSToZRI) plus [the slides](https://speakerdeck.com/rmcelreath)
  - More inspiration from McElreath's blog post [Multilevel Regression as Default](http://elevanth.org/blog/2017/08/24/multilevel-regression-as-default/)


### Code Sources

- Code for this case study comes from two sources
  - McElreath, Chapter 12 code
    - He just sent me his code after I purchased his book (and the pdf!)
    - Can also get draft versions of code [shared for the upcoming 2nd edition of his book](http://xcelab.net/rmpubs/sr2/code.txt)
  - Solomon Kurz, Chapter 12 code from his online book [Statistical Rethinking with brms, ggplot2, and the tidyverse](https://bookdown.org/ajkurz/Statistical_Rethinking_recoded/)
    - Kurz's book is "a love letter" to Statistical Rethinking
    - It goes through McElreath's book and redoes the models using `brms` and the `tidyverse`
    
    
```{r setup, eval=FALSE}
## install rethinking experimental branch
install.packages(c("coda","mvtnorm","devtools","loo"))
library(devtools)
devtools::install_github("rmcelreath/rethinking",ref="Experimental")

## install brms, rstanarm, rstan, tidyverse, ggthemes, tictoc
install.packages(c('brms', 'rstanarm', 'rstan', 'tidyverse', 'ggthemes', 'tictoc))


```
    
## Example Part 1: Fixed Effects Model

### Fixed vs Random (Pooling) Effects Models Refresher

- A robot visits a cafe and orders coffee in Paris...
  - "Fixed Effects" models analagous to *anmestic* robot (forgets)
  - "Random Effects" models analagous to *nmestic* robot (remembers)

### Un-pooled Model

Let's get the `reedfrogs` data from rethinking.

```{r, message = F, warning = F}
## load rstan
library(rstan)
data('reedfrogs', package = 'rethinking')
d <- reedfrogs
rm('reedfrogs')
```

Making the `tank` cluster variable is easy. Keep as integer since `Stan` breaks if grouping variables are not integers.

```{r}
d %>%
  head(10)

d <- d %>%
  dplyr::mutate(tank = 1:nrow(d))

```

Here's the formula for the un-pooled model in which each `tank` gets its own intercept ("fixed effects").

\begin{align*}
\text{surv}_i        & \sim \text{Binomial} (n_i, p_i) \\
\text{logit} (p_i)   & = \alpha_{\text{tank}_i} \\
\alpha_{\text{tank}} & \sim \text{Normal} (0, 5)
\end{align*}

And $n_i = \text{density}_i$. Now we'll fit this simple aggregated binomial model (see Chapter 10 of Kunz or McElreath).

Building models with `rethinking` requires being explicit and specific about model parameters, emulating the way it is written out in mathematical notation.


### `rethinking`

`rethinking` models interface with stan by translating the sytnax into Stan code, compiling, then sampling. We'll me using the `ulam` function from the 2nd edition of the book. The 1st edition used `map2stan` which was more user friendly but had a less flexible syntax. 

> Stan breaks if you send more data than what is actually used by the model (`declare_all_data = F` option required to work)

```{r m12.1, cache = T, message = F, warning = F}
tictoc::tic()
set.seed(12)
m12.1 <- rethinking::ulam(
    alist(
        surv ~ binomial( density , p ) ,
        logit(p) <- a_tank[tank] , # a_tank[tank] = "parameter a_tank grouped by [tank]"
        a_tank[tank] ~ normal( 0 , 5 )
    ),
    data = d,
    declare_all_data = FALSE, # only keep data used in model
    iter = 2000, warmup = 500, chains = 4, cores = 4)
tictoc::toc()
```

### `brms`
`brms` is similar to `rethinking` in that it translates the model to Stan code and compiles. The formula syntax, however, follows more traditional R formula model syntax, intentionally designed to emulate the formulate syntax of the popular `lme4` package, which also fits Random Effects models but using flat priors and maximum likelihood (not bayes).

```{r b12.1, cache = T, message = F, warning = F}
tictoc::tic()
b12.1 <- 
  brms::brm(
    surv | trials(density) ~ 0 + factor(tank),
    data = d, family = binomial,
    prior = brms::prior(normal(0, 5), class = b),
    iter = 2000, warmup = 500, chains = 4, cores = 4,
    seed = 12)
tictoc::toc()
```

### `rstanarm`

The same model but with `rstanarm`. Starts faster because it runs *pre-complied* Stan code. Has a formula syntax closer to that of `lme4` and `brms` but requires specific functions calls to unlock the pre-compiled magic of the package. For example, if you want to run a simple linear model without random effects, then run `stan_lm`. If you want to run generalized linear models without random effects, then use `stan_glm`. For folks that use R, the suffixes `_lm` and `_glm` will be very familiar.

```{r a12.1, cache = T, message = F, warning = F}
tictoc::tic()
a12.1 <- 
  rstanarm::stan_glm(
    cbind(surv, density - surv) ~ 0 + as.factor(tank),
    data = d, family = binomial("logit"),
    prior = rstanarm::normal(0,5),
    iter = 2000, warmup = 500, chains = 4, cores = 4,
    seed = 12)
tictoc::toc()
```


Compare the model coefficient medians for the fixed effects model.

```{r compare-12.1, cache = T, message = F, warning = F}
coef_mat <- cbind(coef(m12.1), brms::fixef(b12.1)[,1], coef(a12.1)) %>%
  'rownames<-'(sprintf('tank[%d]', 1:nrow(d))) %>%
  'colnames<-'(c('m', 'b', 'a'))
coef_mat
```

### Multilevel Alternative aka "Pooling" aka "Random Effects"

The formula for the multilevel alternative is

\begin{align*}
\text{surv}_i        & \sim \text{Binomial} (n_i, p_i) \\
\text{logit} (p_i)   & = \alpha_{\text{tank}_i} \\
\alpha_{\text{tank}} & \sim \text{Normal} (\alpha, \sigma) \\
\alpha               & \sim \text{Normal} (0, 1) \\
\sigma               & \sim \text{HalfCauchy} (0, 1)
\end{align*}

`rethinking` random effects models are specified by assigning *hyperparameters* to original prior: `a_tank[tank] ~ normal(0,5)` becomes `a_tank[tank] ~ normal(a, sigma)` where `a` and `sigma` are the parameters for each tank's intercepts. However, these parameters themselves have priors aka *hyperpriors*. This adds a *second* level to the model---hence, it is a *multilevel* model.

```{r m12.2, cache = T, message = F, warning = F}
tictoc::tic()
set.seed(12)
m12.2 <- 
  rethinking::ulam(
    alist(
        surv ~ binomial( density , p ) ,
        logit(p) <- a_tank[tank] ,
        # a_tank[tank] ~ normal( 0 , 5 ) , (before)
        a_tank[tank] ~ normal( a , sigma ) , 
        a ~ normal(0,1) , # hyperparameter for mean
        sigma ~ cauchy(0,1) # hyperparameter for group dispersion
    ), 
    data=d, declare_all_data = FALSE,
    iter = 4000, warmup = 1000, chains = 4, cores = 4)
tictoc::toc()
```

Here is the same model using `brms`.

The syntax for the varying (random) effects follows the [lme4 style](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf), `( <varying parameter(s)> | <grouping variable(s)> )`. In this case `(1 | tank)` indicates only the intercept, `1`, varies by `tank`. The extent to which parameters vary is controlled by the prior, `prior(cauchy(0, 1), class = sd)`, which is <u>parameterized in the standard deviation metric</u>. Do note that last part. It's common in multilevel software to model in the variance metric, instead. 


```{r b12.2, cache = T, message = F, warning = F}
tictoc::tic()
b12.2 <- 
  brms::brm(
    surv | trials(density) ~ 1 + (1 | tank),
    data = d, family = binomial,
    prior = c(brms::prior(normal(0, 1), class = Intercept),
              brms::prior(cauchy(0, 1), class = sd)),
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 12)
tictoc::toc()
```


`rstanarm` will again be very similar. However, note that are now using `stan_glmer` (note the `er` in `glmer`). This specifies that want to use a model with varying (random) effects. These models *require* that a grouping variable be specified i.e `( <varying parameter> | <grouping variable> )`.

```{r a12.2, cache = T, message = F, warning = F}
tictoc::tic()
a12.2 <- 
  rstanarm::stan_glmer(
    cbind(surv, density - surv) ~ 1 + (1 | tank),
    data = d, family = binomial("logit"),
    prior_intercept = rstanarm::normal(0,1),
    prior = rstanarm::normal(0,1),
    iter = 4000, warmup = 1000, chains = 4, cores = 4,
    seed = 12)
tictoc::toc()
```


```{r compare-12.2, cache = T, message = F, warning = F}
# extracting the coefficients is a little trickier with brms when using pooled models
coef_m <- coef(m12.2)
# brms
coef_b <- c(coef(b12.2)$tank[,1,], brms::fixef(b12.2)[1], sd(brms::ranef(b12.2)$tank[,,1]))
# rstanarm
smry_a <- a12.2$stan_summary
coef_a <- c(coef(a12.2)$tank[,1], rstanarm::fixef(a12.2), smry_a[,'sd'][grepl('_NEW', names(smry_a[,'sd']))])
coef_mat2 <- cbind(coef_m, coef_b, coef_a) %>%
  'rownames<-'(c(sprintf('tank[%d]', 1:nrow(d)), 'a', 'sigma')) %>%
  'colnames<-'(c('m', 'b', 'a'))
coef_mat2
```

### Plotting: `rethinking` base R vs `brms` with `ggplot2`
Finally, here's the ggplot2 code to reproduce Figure 12.1.

```{r, fig.width = 5, fig.height = 4}
post_mdn %>%
  ggplot(aes(x = tank)) +
  geom_hline(yintercept = inv_logit_scaled(median(post$b_Intercept)), linetype = 2, size = 1/4) +
  geom_vline(xintercept = c(16.5, 32.5), size = 1/4) +
  geom_point(aes(y = propsurv), color = "orange2") +
  geom_point(aes(y = post_mdn), shape = 1) +
  coord_cartesian(ylim = c(0, 1)) +
  scale_x_continuous(breaks = c(1, 16, 32, 48)) +
  labs(title    = "Multilevel shrinkage!",
       subtitle = "The empirical proportions are in orange while the model-\nimplied proportions are the black circles. The dashed line is\nthe model-implied average survival proportion.") +
  annotate("text", x = c(8, 16 + 8, 32 + 8), y = 0, 
           label = c("small tanks", "medium tanks", "large tanks")) +
  theme_fivethirtyeight() +
  theme(panel.grid = element_blank())
```

Here is our version of Figure 12.2.a. 

```{r, fig.width = 3, fig.height = 3}
# this makes the output of `sample_n()` reproducible
set.seed(12)

post %>% 
  sample_n(100) %>% 
  expand(nesting(iter, b_Intercept, sd_tank__Intercept),
         x = seq(from = -4, to = 5, length.out = 100)) %>% 

  ggplot(aes(x = x, group = iter)) +
  geom_line(aes(y = dnorm(x, b_Intercept, sd_tank__Intercept)),
            alpha = .2, color = "orange2") +
  labs(title = "Population survival distribution",
       subtitle = "The Gaussians are on the log-odds scale.") +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(-3, 4)) + 
  theme_fivethirtyeight() +
  theme(plot.title    = element_text(size = 13),
        plot.subtitle = element_text(size = 10))
```

Note the uncertainty in terms of both location $\alpha$ and scale $\sigma$. Now here's the code for Figure 12.2.b.

```{r, fig.width = 3, fig.height = 3}
ggplot(data = post, 
       aes(x = rnorm(n    = nrow(post), 
                     mean = b_Intercept, 
                     sd   = sd_tank__Intercept) %>% 
             inv_logit_scaled())) +
  geom_density(size = 0, fill = "orange2") +
  scale_y_continuous(NULL, breaks = NULL) +
  ggtitle("Probability of survival") +
  theme_fivethirtyeight()
```

Note how we sampled 12,000 imaginary `tanks` rather than McElreath's 8,000. This is because we had 12,000 HMC iterations (i.e., execute `nrow(post)`).

The `aes()` code, above, was a bit much. To get a sense of how it worked, consider this:

```{r}
set.seed(12)

rnorm(n    = 1, 
      mean = post$b_Intercept, 
      sd   = post$sd_tank__Intercept) %>% 
  inv_logit_scaled()
```

First, we took one random draw from a normal distribution with a mean of the first row in `post$b_Intercept` and a standard deviation of the value from the first row in `post$sd_tank__Intercept`, and passed it through the `inv_logit_scaled()` function. By replacing the `1` with `nrow(post)`, we do this `nrow(post)` times (i.e., 12,000). Our orange density, then, is the summary of that process.

#### Overthinking: Prior for variance components.

Yep, you can use the exponential distribution for your priors in brms. Here it is for model `b12.2`.

```{r b12.2.e, cache = T, message = F, results = "hide"}
b12.2.e <- 
  update(b12.2,
         prior = c(prior(normal(0, 1), class = Intercept),
                   prior(exponential(1), class = sd)))
```

The model summary:

```{r}
print(b12.2.e)
```

If you're curious how the exponential prior compares to the posterior, you might just plot.

```{r, fig.width = 3.75, fig.height = 3.25}
tibble(x = seq(from = 0, to = 6, by = .01)) %>% 
  
  ggplot() +
  # the prior
  geom_ribbon(aes(x = x, ymin = 0, ymax = dexp(x, rate = 1)),
              fill = "orange2", alpha = 1/3) +
  # the posterior
  geom_density(data = posterior_samples(b12.2.e),
               aes(x = sd_tank__Intercept), 
               fill = "orange2", size = 0) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = c(0, 5)) +
  labs(title    = "Bonus prior/posterior plot\nfor sd_tank__Intercept",
       subtitle = "The prior is the semitransparent ramp in the\nbackground. The posterior is the solid orange\nmound.") +
  theme_fivethirtyeight()
```  

## Varying effects and the underfitting/overfitting trade-off

> Varying intercepts are just regularized estimates, but adaptively regularized by estimating how diverse the clusters are while estimating the features of each cluster. This fact is not easy to grasp…
>
> A major benefit of using varying effects estimates, instead of the empirical raw estimates, is that they provide more accurate estimates of the individual cluster (tank) intercepts. On average, the varying effects actually provide a better estimate of the individual tank (cluster) means. The reason that the varying intercepts provides better estimates is that they do a better job trading off underfitting and overfitting. (p. 364)

In this section, we explicate this by contrasting three perspectives:

* Complete pooling (i.e., a single-$\alpha$ model)
* No pooling (i.e., the single-level $\alpha_{\text{tank}_i}$ model)
* Partial pooling (i.e., the multilevel model for which $\alpha_{\text{tank}} \sim \text{Normal} (\alpha, \sigma)$)

> To demonstrate [the magic of the multilevel model], we’ll simulate some tadpole data. That way, we’ll know the true per-pond survival probabilities. Then we can compare the no-pooling estimates to the partial pooling estimates, by computing how close each gets to the true values they are trying to estimate. The rest of this section shows how to do such a simulation. (p. 365)

### The model.

The simulation formula should look familiar.

\begin{align*}
\text{surv}_i        & \sim \text{Binomial} (n_i, p_i) \\
\text{logit} (p_i)   & = \alpha_{\text{pond}_i} \\
\alpha_{\text{pond}} & \sim \text{Normal} (\alpha, \sigma) \\
\alpha               & \sim \text{Normal} (0, 1) \\
\sigma               & \sim \text{HalfCauchy} (0, 1)
\end{align*}

### Assign values to the parameters.

```{r}
a       <-  1.4
sigma   <-  1.5
n_ponds <- 60

set.seed(12)
(
  dsim <- 
  tibble(pond   = 1:n_ponds,
         ni     = rep(c(5, 10, 25, 35), each = n_ponds / 4) %>% as.integer(),
         true_a = rnorm(n = n_ponds, mean = a, sd = sigma))
  )
```

### Sumulate survivors.

> Each pond $i$ has $n_i$ potential survivors, and nature flips each tadpole’s coin, so to speak, with probability of survival $p_i$. This probability $p_i$ is implied by the model definition, and is equal to:
>
> $$p_i = \frac{\text{exp} (\alpha_i)}{1 + \text{exp} (\alpha_i)}$$
>
> The model uses a logit link, and so the probability is defined by the [`inv_logit_scaled()`] function. (p. 367)

```{r}
set.seed(12)
(
  dsim <-
  dsim %>%
  mutate(si = rbinom(n = n(), prob = inv_logit_scaled(true_a), size = ni))
  )
```

### Compute the no-pooling estimates.

The no-pooling estimates (i.e., $\alpha_{\text{tank}_i}$) are the results of simple algebra.

```{r}
(
  dsim <-
  dsim %>%
  mutate(p_nopool = si / ni)
  )
```

"These are the same no-pooling estimates you’d get by fitting a model with a dummy variable for each pond and flat priors that induce no regularization" (p. 367).

### Compute the partial-pooling estimates.

To follow along with McElreath, set `chains = 1, cores = 1` to fit with one chain.

```{r b12.3, cache = T, message = F, warning = F, results = "hide"}
b12.3 <- 
  brm(data = dsim, family = binomial,
      si | trials(ni) ~ 1 + (1 | pond),
      prior = c(prior(normal(0, 1), class = Intercept),
                prior(cauchy(0, 1), class = sd)),
      iter = 10000, warmup = 1000, chains = 1, cores = 1,
      seed = 12)
```

```{r}
print(b12.3)
```

I'm not aware that you can use McElreath's `depth=2` trick in brms for `summary()` or `print()`. But can get that information with the `coef()` function. 

```{r}
coef(b12.3)$pond[c(1:2, 59:60), , ] %>% 
  round(digits = 2)
```

Note how we just peeked at the top and bottom two rows with the `c(1:2, 59:60)` part of the code, there. Somewhat discouragingly, `coef()` doesn’t return the 'Eff.Sample' or 'Rhat' columns as in McElreath’s output. We can still extract that information, though. For $\hat{R}$, the solution is simple; use the `brms::rhat()` function.

```{r}
rhat(b12.3)
```

Extracting the 'Eff.Sample' values is a little more complicated. There is no `effsamples()` function. However, we do have `neff_ratio()`. 

```{r}
neff_ratio(b12.3)
```

The `brms::neff_ratio()` function returns ratios of the effective samples over the total number of post-warmup iterations. So if we know the `neff_ratio()` values and the number of post-warmup iterations, the 'Eff.Sample' values are just a little algebra away. A quick solution is to look at the 'total post-warmup samples' line at the top of our `print()` output. Another way is to extract that information from our `brm()` fit object. I’m not aware of a way to do that directly, but we can extract the `iter` value (i.e., `b12.2$fit@sim$iter`), the `warmup` value (i.e., `b12.2$fit@sim$warmup`), and the number of `chains` (i.e., `b12.2$fit@sim$chains`). With those values in hand, simple algebra will return the 'total post-warmup samples' value. E.g.,

```{r}
(n_iter <- (b12.3$fit@sim$iter - b12.3$fit@sim$warmup) * b12.3$fit@sim$chains)
```

And now we have `n_iter`, we can calculate the 'Eff.Sample' values.

```{r}
neff_ratio(b12.3) %>% 
  data.frame() %>% 
  rownames_to_column() %>% 
  set_names("parameter", "neff_ratio") %>% 
  mutate(eff_sample = (neff_ratio * n_iter) %>% round(digits = 0)) %>% 
  head()
```

Digressions aside, let's get ready for the diagnostic plot of Figure 12.3.

```{r}
dsim %>% 
  glimpse()
```

```{r}
# we could have included this step in the block of code below, if we wanted to
p_partpool <- 
  coef(b12.3)$pond[, , ] %>% 
  as_tibble() %>%
  transmute(p_partpool = inv_logit_scaled(Estimate))

dsim <- 
  dsim %>%
  bind_cols(p_partpool) %>% 
  mutate(p_true         = inv_logit_scaled(true_a)) %>%
  mutate(nopool_error   = abs(p_nopool   - p_true),
         partpool_error = abs(p_partpool - p_true))

dsim %>% 
  glimpse()
```

Here is our code for Figure 12.3. The extra data processing for `dfline` is how we get the values necessary for the horizontal summary lines.

```{r, fig.width = 5, fig.height = 5}
dfline <- 
  dsim %>%
  select(ni, nopool_error:partpool_error) %>%
  gather(key, value, -ni) %>%
  group_by(key, ni) %>%
  summarise(mean_error = mean(value)) %>%
  mutate(x    = c( 1, 16, 31, 46),
         xend = c(15, 30, 45, 60))
  
dsim %>% 
  ggplot(aes(x = pond)) +
  geom_vline(xintercept = c(15.5, 30.5, 45.4), 
             color = "white", size = 2/3) +
  geom_point(aes(y = nopool_error), color = "orange2") +
  geom_point(aes(y = partpool_error), shape = 1) +
  geom_segment(data = dfline, 
               aes(x = x, xend = xend, 
                   y = mean_error, yend = mean_error),
               color = rep(c("orange2", "black"), each = 4),
               linetype = rep(1:2, each = 4)) +
  scale_x_continuous(breaks = c(1, 10, 20, 30, 40, 50, 60)) +
  annotate("text", x = c(15 - 7.5, 30 - 7.5, 45 - 7.5, 60 - 7.5), y = .45, 
           label = c("tiny (5)", "small (10)", "medium (25)", "large (35)")) +
  labs(y        = "absolute error",
       title    = "Estimate error by model type",
       subtitle = "The horizontal axis displays pond number. The vertical axis measures\nthe absolute error in the predicted proportion of survivors, compared to\nthe true value used in the simulation. The higher the point, the worse\nthe estimate. No-pooling shown in orange. Partial pooling shown in black.\nThe orange and dashed black lines show the average error for each kind\nof estimate, across each initial density of tadpoles (pond size). Smaller\nponds produce more error, but the partial pooling estimates are better\non average, especially in smaller ponds.") +
  theme_fivethirtyeight() +
  theme(panel.grid    = element_blank(),
        plot.subtitle = element_text(size = 10))
```

If you wanted to quantify the difference in simple summaries, you might do something like this:

```{r}
dsim %>%
  select(ni, nopool_error:partpool_error) %>%
  gather(key, value, -ni) %>%
  group_by(key) %>%
  summarise(mean_error   = mean(value) %>% round(digits = 3),
            median_error = median(value) %>% round(digits = 3))
```


## Other Bayesian (Stan) Resources

- [Online Python book](https://github.com/pymc-devs/resources/tree/master/Rethinking) for python folks out there    
- [Rasmus Bååth](http://www.sumsar.net) who has a nice course on [Datacamp](http://www.sumsar.net/blog/2018/12/my-introductory-course-on-bayesian-statistics/)
- [Michael Betancourt](https://betanalpha.github.io) who has great [case studies](https://betanalpha.github.io/writing/) to learn from
- [Stan Reference Manual](https://mc-stan.org/docs/2_19/stan-users-guide/index.html) is amazing



